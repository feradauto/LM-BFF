{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a881b978",
   "metadata": {},
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f85d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60622cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu110'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3e12070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text(template, input_text_tuple, label, tokenizer, mapping):\n",
    "    def enc(text):\n",
    "        return tokenizer.encode(text, add_special_tokens=False)\n",
    "    special_token_mapping = {'cls': tokenizer.cls_token_id, 'mask': tokenizer.mask_token_id, 'sep': tokenizer.sep_token_id, 'sep+': tokenizer.sep_token_id}\n",
    "    for i in range(10):\n",
    "        special_token_mapping[\"<extra_id_%d>\" % (i)] = tokenizer._convert_token_to_id(\"<extra_id_%d>\" % (i))\n",
    "    template_list = template.split('*')\n",
    "    input_ids = []\n",
    "    for part in template_list:\n",
    "        new_tokens = []\n",
    "        if part in special_token_mapping:\n",
    "            if part == 'cls' and 'T5' in type(tokenizer).__name__:\n",
    "                # T5 does not have cls token\n",
    "                continue\n",
    "            new_tokens.append(special_token_mapping[part])\n",
    "        elif part[:5] == 'label':\n",
    "            new_tokens += enc(' ' + label)\n",
    "        elif part[:5] == 'sent_':\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            new_tokens += enc(input_text_tuple[sent_id])\n",
    "        elif part[:6] == '+sent_':\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            new_tokens += enc(' ' + input_text_tuple[sent_id]) # add space\n",
    "        elif part[:6] == 'sent-_':\n",
    "            # Delete the last token\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            new_tokens += enc(input_text_tuple[sent_id][:-1])\n",
    "        elif part[:7] == '+sentl_':\n",
    "            # Lower case the first token\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            text = input_text_tuple[sent_id]\n",
    "            text = text[:1].lower() + text[1:]\n",
    "            new_tokens += enc(' ' + text)\n",
    "        elif part[:7] == '+sentu_':\n",
    "            # Upper case the first token\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            text = input_text_tuple[sent_id]\n",
    "            text = text[:1].upper() + text[1:]\n",
    "            new_tokens += enc(' ' + text)\n",
    "        elif part[:6] == 'sentl_':\n",
    "            # Lower case the first token\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            text = input_text_tuple[sent_id]\n",
    "            text = text[:1].lower() + text[1:]\n",
    "            new_tokens += enc(text)\n",
    "        elif part[:6] == 'sentu_':\n",
    "            # Lower case the first token\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            text = input_text_tuple[sent_id]\n",
    "            text = text[:1].upper() + text[1:]\n",
    "            new_tokens += enc(text)\n",
    "        elif part[:7] == 'sentl-_':\n",
    "            # Lower case the first token\n",
    "            sent_id = int(part.split('_')[1])\n",
    "            text = input_text_tuple[sent_id]\n",
    "            text = text[:1].lower() + text[1:]\n",
    "            new_tokens += enc(text[:-1])\n",
    "        else:\n",
    "            part = part.replace('_', ' ') # there cannot be space in command, so use '_' to replace space\n",
    "            # handle special case when t5 tokenizer might add an extra space\n",
    "            if len(part) == 1:\n",
    "                new_tokens.append(tokenizer._convert_token_to_id(part))\n",
    "            else:\n",
    "                new_tokens += enc(part)\n",
    "\n",
    "        input_ids += new_tokens\n",
    "    return input_ids\n",
    "\n",
    "def generate(dataset, template, model, tokenizer, target_number, mapping, beam, label=None, length_limit=None, truncate=None):\n",
    "    \"\"\"\n",
    "    Generate templates based on given inputs\n",
    "\n",
    "    label: Only use instances with this label (deprecated)\n",
    "    length_limit: At least generate content as long as length_limit (deprecated)\n",
    "    \"\"\"\n",
    "    input_texts = []\n",
    "    input_tensors = []\n",
    "    max_length = 0\n",
    "\n",
    "    # Process the inputs\n",
    "    for item in dataset:\n",
    "        if label is None or item['label'] == label:\n",
    "            if type(item['label'])==float and math.isnan(item['label']):\n",
    "                item['label']=''\n",
    "            input_text = get_text(template, item['text'], item['label'], tokenizer, mapping)\n",
    "            if truncate is not None:\n",
    "                if truncate == 'head':\n",
    "                    input_text = input_text[-256:]\n",
    "                elif truncate == 'tail':\n",
    "                    input_text = input_text[:256]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            input_ids = torch.tensor(input_text).long()\n",
    "            max_length = max(max_length, input_ids.size(-1))\n",
    "            input_tensors.append(input_ids)\n",
    "\n",
    "    # Concatenate inputs as a batch\n",
    "    input_ids = torch.zeros((len(input_tensors), max_length)).long()\n",
    "    attention_mask = torch.zeros((len(input_tensors), max_length)).long()\n",
    "    for i in range(len(input_tensors)):\n",
    "        input_ids[i, :input_tensors[i].size(-1)] = input_tensors[i]\n",
    "        attention_mask[i, :input_tensors[i].size(-1)] = 1\n",
    "\n",
    "    # Print some examples\n",
    "    print('####### example #######')\n",
    "    print(tokenizer.decode(input_ids[0]))\n",
    "    print(tokenizer.decode(input_ids[1]))\n",
    "    print(tokenizer.decode(input_ids[2]))\n",
    "    print('####### example #######\\n')\n",
    "\n",
    "    input_ids = input_ids.cuda()\n",
    "    attention_mask = attention_mask.cuda()\n",
    "    assert len(input_tensors) > 0\n",
    "\n",
    "    # Maximum generate content length\n",
    "    max_length = 20\n",
    "\n",
    "    start_mask = tokenizer._convert_token_to_id('<extra_id_0>')\n",
    "    ori_decoder_input_ids = torch.zeros((input_ids.size(0), max_length)).long()\n",
    "    ori_decoder_input_ids[..., 0] = model.config.decoder_start_token_id\n",
    "\n",
    "    # decoder_input_ids: decoder inputs for next regressive generation\n",
    "    # ll: log likelihood\n",
    "    # output_id: which part of generated contents we are at\n",
    "    # output: generated content so far\n",
    "    # last_length (deprecated): how long we have generated for this part\n",
    "    current_output = [{'decoder_input_ids': ori_decoder_input_ids, 'll': 0, 'output_id': 1, 'output': [], 'last_length': -1}]\n",
    "    for i in tqdm(range(max_length - 2)):\n",
    "        new_current_output = []\n",
    "        for item in current_output:\n",
    "            if item['output_id'] > target_number:\n",
    "                # Enough contents\n",
    "                new_current_output.append(item)\n",
    "                continue\n",
    "            decoder_input_ids = item['decoder_input_ids']\n",
    "\n",
    "            # Forward\n",
    "            batch_size = 32\n",
    "            turn = input_ids.size(0) // batch_size\n",
    "            if input_ids.size(0) % batch_size != 0:\n",
    "                turn += 1\n",
    "            aggr_output = []\n",
    "            for t in range(turn):\n",
    "                start = t * batch_size\n",
    "                end = min((t + 1) * batch_size, input_ids.size(0))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    aggr_output.append(model(input_ids[start:end], attention_mask=attention_mask[start:end], decoder_input_ids=decoder_input_ids.cuda()[start:end])[0])\n",
    "            aggr_output = torch.cat(aggr_output, 0)\n",
    "\n",
    "            # Gather results across all input sentences, and sort generated tokens by log likelihood\n",
    "            aggr_output = aggr_output.mean(0)\n",
    "            log_denominator = torch.logsumexp(aggr_output[i], -1).item()\n",
    "            ids = list(range(model.config.vocab_size))\n",
    "            ids.sort(key=lambda x: aggr_output[i][x].item(), reverse=True)\n",
    "            ids = ids[:beam+3]\n",
    "            \n",
    "            for word_id in ids:\n",
    "                output_id = item['output_id']\n",
    "\n",
    "                if word_id == start_mask - output_id or word_id == tokenizer._convert_token_to_id('</s>'):\n",
    "                    # Finish one part\n",
    "                    if length_limit is not None and item['last_length'] < length_limit[output_id - 1]:\n",
    "                        check = False\n",
    "                    else:\n",
    "                        check = True\n",
    "                    output_id += 1\n",
    "                    last_length = 0\n",
    "                else:\n",
    "                    last_length = item['last_length'] + 1\n",
    "                    check = True\n",
    "\n",
    "                output_text = item['output'] + [word_id]\n",
    "                ll = item['ll'] + aggr_output[i][word_id] - log_denominator\n",
    "                new_decoder_input_ids = decoder_input_ids.new_zeros(decoder_input_ids.size())\n",
    "                new_decoder_input_ids[:] = decoder_input_ids\n",
    "                new_decoder_input_ids[..., i + 1] = word_id\n",
    "\n",
    "                # Forbid single space token, \"....\", and \"..........\"\n",
    "                if word_id in [3, 19794, 22354]:\n",
    "                    check = False\n",
    "\n",
    "                # Forbid continuous \".\"\n",
    "                if len(output_text) > 1 and output_text[-2] == 5 and output_text[-1] == 5:\n",
    "                    check = False\n",
    "\n",
    "                if check:\n",
    "                    # Add new results to beam search pool\n",
    "                    new_item = {'decoder_input_ids': new_decoder_input_ids, 'll': ll, 'output_id': output_id, 'output': output_text, 'last_length': last_length}\n",
    "                    new_current_output.append(new_item)\n",
    "\n",
    "        if len(new_current_output) == 0:\n",
    "            break\n",
    "\n",
    "        new_current_output.sort(key=lambda x: x['ll'], reverse=True)\n",
    "        new_current_output = new_current_output[:beam]\n",
    "        current_output = new_current_output\n",
    "\n",
    "    result = []\n",
    "    print(\"####### generated results #######\")\n",
    "    for item in current_output:\n",
    "        generate_text = ''\n",
    "        for token in item['output']:\n",
    "            generate_text += tokenizer._convert_id_to_token(token)\n",
    "        print('--------------')\n",
    "        print('score:', item['ll'].item())\n",
    "        print('generated ids', item['output'])\n",
    "        print('generated text', generate_text)\n",
    "        result.append(generate_text)\n",
    "    print(\"####### generated results #######\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def load_dataset(task, data_dir):\n",
    "    if task in [\"MNLI\", \"MRPC\", \"QNLI\", \"QQP\", \"RTE\", \"SNLI\", \"SST-2\", \"STS-B\", \"WNLI\", \"CoLA\"]:\n",
    "        lines = open(os.path.join(data_dir, 'train.tsv')).readlines()\n",
    "        if task != 'CoLA':\n",
    "            lines = lines[1:]\n",
    "\n",
    "        dataset = []\n",
    "        for line in lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            if task == 'CoLA':\n",
    "                dataset.append({'label': line[1], 'text': [line[-1]]})\n",
    "            elif task == 'MNLI':\n",
    "                dataset.append({'label': line[-1], 'text': [line[8], line[9]]})\n",
    "            elif task == 'MRPC':\n",
    "                dataset.append({'label': line[0], 'text': [line[-2], line[-1]]})\n",
    "            elif task == 'QNLI':\n",
    "                dataset.append({'label': line[-1], 'text': [line[1], line[2]]})\n",
    "            elif task == 'QQP':\n",
    "                dataset.append({'label': line[-1], 'text': [line[3], line[4]]})\n",
    "            elif task == 'RTE':\n",
    "                dataset.append({'label': line[-1], 'text': [line[1], line[2]]})\n",
    "            elif task == 'SNLI':\n",
    "                dataset.append({'label': line[-1], 'text': [line[7], line[8]]})\n",
    "            elif task == 'SST-2':\n",
    "                dataset.append({'label': line[-1], 'text': [line[0]]})\n",
    "            elif task == 'STS-B':\n",
    "                dataset.append({'label': '0' if float(line[-1]) < 2.5 else '1', 'text': [line[-3], line[-2]]})\n",
    "            elif task == 'WNLI':\n",
    "                dataset.append({'label': line[-1], 'text': [line[1], line[2]]})\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "    else:\n",
    "        lines = pd.read_csv(os.path.join(data_dir, 'train.csv')).values.tolist()\n",
    "        dataset = []\n",
    "        for line in lines:\n",
    "            dataset.append({'label': line[0], 'text': [line[1]]})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def search_template(model, tokenizer, task_name, k, seed, beam, output_dir, data_dir):\n",
    "    print('#', task_name, k, seed, beam)\n",
    "    dataset_path = os.path.join(data_dir, task_name)#, \"{}-{}\".format(k, seed))\n",
    "    dataset = load_dataset(task_name, dataset_path)\n",
    "    print('|', 'dataset examples')\n",
    "    print('|', dataset[0])\n",
    "    print('|', dataset[-1])\n",
    "    print()\n",
    "    \n",
    "    # Manual label word mappings\n",
    "    map_of_mapping = {\n",
    "        'SST-2': {'0':'terrible','1':'great'},\n",
    "        'sst-5': {0:'terrible',1:'bad',2:'okay',3:'good',4:'great'},\n",
    "        'mr': {0:'terrible',1:'great'},\n",
    "        'cr': {0:'terrible',1:'great'},\n",
    "        'subj': {0:'subjective',1:'objective'},\n",
    "        'trec': {0:'Description',1:'Entity',2:'Expression',3:'Human',4:'Location',5:'Number'},\n",
    "        'mpqa': {0:'terrible',1:'great'},\n",
    "        'CoLA': {'0':'incorrect','1':'correct'},\n",
    "        'MRPC': {'0':'No','1':'Yes'},\n",
    "        'QQP': {'0':'No','1':'Yes'},\n",
    "        'STS-B': {'0':'No','1':'Yes'},\n",
    "        'MNLI': {'contradiction':'No','entailment':'Yes','neutral':'Maybe'},\n",
    "        'SNLI': {'contradiction':'No','entailment':'Yes','neutral':'Maybe'},\n",
    "        'QNLI': {'not_entailment':'No','entailment':'Yes'},\n",
    "        'RTE': {'not_entailment':'No','entailment':'Yes'}\n",
    "    }\n",
    "\n",
    "    mapping = {}#map_of_mapping[task_name]\n",
    "    print('|', 'mapping')\n",
    "    print('|', mapping)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, task_name), exist_ok=True)\n",
    "    f = open(os.path.join(output_dir, task_name, \"{}-{}.txt\".format(k, seed)), 'w')\n",
    "\n",
    "    if task_name in ['SST-2', 'sst-5', 'mr', 'cr', 'subj', 'trec', 'CoLA', 'mpqa','papers']:\n",
    "        # Single sentence tasks\n",
    "        # We take two kinds of templates: put [MASK] at the beginning or the end\n",
    "        template = \"*cls**sentu_0**<extra_id_0>**label**<extra_id_1>**sep+*\"\n",
    "        generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None, truncate='head')[:beam//2]\n",
    "\n",
    "        print(\"####### generated templates #######\")\n",
    "        for text in generate_text:\n",
    "            # Transform T5 outputs to our template format\n",
    "            text = text.replace('<extra_id_0>', '*cls**sent_0*')\n",
    "            text = text.replace('<extra_id_1>', '*mask*')\n",
    "            text = text.replace('<extra_id_2>', '*sep+*')\n",
    "            text = text.replace('</s>', '*sep+*')\n",
    "            text = text.replace('▁', '_')\n",
    "            print(text)\n",
    "            f.write(text + '\\n')\n",
    "        print(\"####### generated templates #######\\n\")\n",
    "\n",
    "        template = \"*cls*.*<extra_id_0>**label**<extra_id_1>**+sentu_0**sep+*\"\n",
    "        generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None, truncate='tail')[:beam//2]\n",
    "        print(\"####### generated templates #######\")\n",
    "        for text in generate_text:\n",
    "            # Transform T5 outputs to our template format\n",
    "            text = text.replace('<extra_id_0>', '*cls*')\n",
    "            text = text.replace('<extra_id_1>', '*mask*')\n",
    "            text = text.replace('<extra_id_2>', '*+sent_0**sep+*')\n",
    "            text = text.replace('</s>', '*+sent_0**sep+*')\n",
    "            text = text.replace('▁', '_')\n",
    "            print(text)\n",
    "            f.write(text + '\\n')\n",
    "        print(\"####### generated templates #######\\n\")\n",
    "\n",
    "    elif task_name in ['MRPC', 'QQP', 'STS-B', 'MNLI', 'SNLI', 'QNLI', 'RTE']:\n",
    "        # Sentence pair tasks\n",
    "        # We always put [MASK] between the two sentences\n",
    "        template = \"*cls**sent-_0**<extra_id_0>**label**<extra_id_1>**+sentl_1**sep+*\"\n",
    "        generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None)\n",
    "        print(\"####### generated templates #######\")\n",
    "        for text in generate_text:\n",
    "            # Transform T5 outputs to our template format\n",
    "            text = text.replace('<extra_id_0>', '*cls**sent-_0*')\n",
    "            text = text.replace('<extra_id_1>', '*mask*')\n",
    "            text = text.replace('<extra_id_2>', '*+sentl_1**sep+*')\n",
    "            text = text.replace('</s>', '*+sentl_1**sep+*')\n",
    "            text = text.replace('▁', '_')\n",
    "            print(text)\n",
    "            f.write(text + '\\n')\n",
    "        print(\"####### generated templates #######\\n\")\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71bd97a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--k'], dest='k', nargs=None, const=None, default=16, type=<class 'int'>, choices=None, help='Number of training instances per label', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--t5_model', type=str, default='t5-small', help='T5 pre-trained model')\n",
    "parser.add_argument('--seed', type=int, nargs='+', default=[42], help=\"Data split seeds\")\n",
    "parser.add_argument('--task_name', type=str, nargs='+', default=['papers'], help=\"Task names\")\n",
    "parser.add_argument('--output_dir', type=str, default='/cluster/scratch/fgonzalez/auto_template')\n",
    "\n",
    "parser.add_argument('--data_dir', type=str, default=\"/cluster/scratch/fgonzalez/prompts/LM-BFF/data/k-shot\", help=\"Data directory\")\n",
    "parser.add_argument('--beam', type=int, default=100, help=\"Beam search width\")\n",
    "parser.add_argument('--k', type=int, default=16, help=\"Number of training instances per label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8cafac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88e4cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(args.t5_model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(args.t5_model)\n",
    "tokenizer.sep_token = '</s>'\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74abf40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name=args.task_name[0]\n",
    "seed = args.seed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22f32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model\n",
    "tokenizer=tokenizer\n",
    "task_name=task_name\n",
    "k=args.k\n",
    "seed=seed\n",
    "beam=args.beam\n",
    "output_dir=args.output_dir\n",
    "data_dir=args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f0d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# papers 16 42 100\n",
      "| dataset examples\n",
      "| {'label': 'semantic image segmentation', 'text': [\"Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems , as an accurate understanding of the surrounding scene is crucial to navigation and action planning . Current state - of - the - art approaches in semantic image segmentation rely on pretrained networks that were initially developed for classifying images as a whole . While these networks exhibit outstanding recognition performance ( i.e. , what is visible ? ) , they lack localization accuracy ( i.e. , where precisely is something located ? ) . Therefore , additional processing steps have to be performed in order to obtain pixel - accurate segmentation masks at the full image resolution . To alleviate this problem we propose a novel ResNet - like architecture that exhibits strong localization and recognition performance . We combine multi - scale context with pixel - level accuracy by using two processing streams within our network : One stream carries information at the full image resolution , enabling precise adherence to segment boundaries . The other stream undergoes a sequence of pooling operations to obtain robust features for recognition . The two streams are coupled at the full image resolution using residuals . Without additional processing steps and without pretraining , our approach achieves an intersection - over - union score of 71.8 % on the Cityscapes dataset . section : Introduction Recent years have seen an increasing interest in self driving cars and in driver assistance systems . A crucial aspect of autonomous driving is to acquire a comprehensive understanding of the surroundings in which a car is moving . Semantic image segmentation [ reference ][ reference ][ reference ][ reference ][ reference ] , the task of assigning a set of predefined class labels to image pixels , is an important tool for modeling the complex relationships of the semantic entities usually found in street scenes , such as cars , pedestrians , road , or sidewalks . In automotive scenarios it is used in various ways , e.g. as a pre - processing step to discard image regions that are unlikely to contain objects of interest [ reference ][ reference ] , to improve object detection [ reference ][ reference ][ reference ][ reference ] . Example output and the abstract structure of our fullresolution residual network . The network has two processing streams . The residual stream ( blue ) stays at the full image resolution , the pooling stream ( red ) undergoes a sequence of pooling and unpooling operations . The two processing streams are coupled using full - resolution residual units [ reference ] or in combination with 3D scene geometry [ reference ][ reference ][ reference ] . Many of those applications require precise region boundaries [ reference ] . In this work , we therefore pursue the goal of achieving high - quality semantic segmentation with precise boundary adherence . Current state - of - the - art approaches for image segmentation all employ some form of fully convolutional network ( FCNs ) [ reference ] that takes the image as input and outputs a probability map for each class . Many papers rely on network architectures that have already been proven successful for image classification such as variants of the ResNet [ reference ] or the VGG architecture [ reference ] . Starting from pre - trained nets , where a large number of weights for the target task can be pre - set by an auxiliary classification task , reduces training time and often yields superior performance compared to training a network from scratch using the ( possibly limited amount of ) data of the target application . However , a main limitation of using such pre - trained networks is that they severely restrict the design space of novel approaches , since new network elements such as batch normalization [ reference ] or new activation functions often can not be added into an existing architecture . When performing semantic segmentation using FCNs , a common strategy is to successively reduce the spatial size of the feature maps using pooling operations or strided convolutions . This is done for two reasons : First , it significantly increases the size of the receptive field and second , it makes the network robust against small translations in the image . While pooling operations are highly desirable for recognizing objects in images , they significantly deteriorate localization performance of the networks when applied to semantic image segmentation . Several approaches exist to overcome this problem and obtain pixel - accurate segmentations . Noh et al . [ reference ] learn a mirrored VGG network as a decoder , Yu and Koltun [ reference ] introduce dilated convolutions to reduce the pooling factor of their pre - trained network . Ghiasi et al . [ reference ] use multi - scale predictions to successively improve their boundary adherence . An alternative approach used by several methods is to apply post - processing steps such as CRF - smoothing [ reference ] . In this paper , we propose a novel network architecture that achieves state - of - the - art segmentation performance without the need for additional post - processing steps and without the limitations imposed by pre - trained architectures . Our proposed ResNet - like architecture unites strong recognition performance with precise localization capabilities by combining two distinct processing streams . One stream undergoes a sequence of pooling operations and is responsible for understanding large - scale relationships of image elements ; the other stream carries feature maps at the full image resolution , resulting in precise boundary adherence . This idea is visualized in Figure 1 , where the two processing streams are shown in blue and red . The blue residual lane reflects the high - resolution stream . It can be combined with classical residual units ( left and right ) , as well as with our new full - resolution residual units ( FRRU ) . The FRRUs from the red pooling lane act as residual units for the blue stream , but also undergo pooling operations and carry high - level information through the network . This results in a network that successively combines and computes features at two resolutions . This paper makes the following contributions : ( i ) We propose a novel network architecture geared towards precise semantic segmentation in street scenes which is not limited to pre - trained architectures and achieves state - ofthe - art results . ( ii ) We propose to use two processing streams to realize strong recognition and strong localization performance : One stream undergoes a sequence of pooling operations while the other stream stays at the full image resolution . ( iii ) In order to foster further research in this area , we publish our code and the trained models in Theano / Lasagne [ reference ][ reference ] 1 . 1 https: // github.com / TobyPDE / FRRN section : Related Work The dramatic performance improvements from using CNNs for semantic segmentation have brought about an increasing demand for such algorithms in the context of autonomous driving scenarios . As a large amount of annotated data is crucial in order to train such deep networks , multiple new datasets have been released to encourage further research in this area , including Synthia [ reference ] , Virtual KITTI [ reference ] , and Cityscapes [ reference ] . In this work , we focus on Cityscapes , a recent large - scale dataset consisting of real - world imagery with well - curated annotations . Given their success , we will constrain our literature review to deep learning based semantic segmentation approaches and deep learning network architectures . Semantic Segmentation Approaches . Over the last years , the most successful semantic segmentation approaches have been based on convolutional neural networks ( CNNs ) . Early approaches constrained their output to a bottom - up segmentation followed by a CNN based region classification [ reference ] . Rather than classifying entire regions in the first place , the approach by Farabet et al . performs pixel - wise classification using CNN features originating from multiple scales , followed by aggregation of these noisy pixel predictions over superpixel regions [ reference ] . The introduction of so - called fully convolutional networks ( FCNs ) for semantic image segmentation by Long et al . [ reference ] opened a wide range of semantic segmentation research using end - to - end training [ reference ] . Long et al . further reformulated the popular VGG architecture [ reference ] as a fully convolutional network ( FCN ) , enabling the use of pretrained models for this architecture . To improve segmentation performance at object boundaries , skip connections were added which allow information to propagate directly from early , high - resolution layers to deeper layers . Pooling layers in FCNs fulfill a crucial role in order to increase the receptive field size of later units and with it the classification performance . However , they have the downside that the resulting network outputs are at a lower resolution . To overcome this , various strategies have been proposed . Some approaches extract features from intermediate layers via some sort of skip connections [ reference ][ reference ][ reference ][ reference ] . Noh et al . propose an encoder / decoder network [ reference ] . The encoder computes low - dimensional feature representations via a sequence of pooling and convolution operations . The decoder , which is stacked on top of the encoder , then learns an upscaling of these low - dimensional features via subsequent unpooling and deconvolution operations [ reference ] . Similarly , Badrinarayanan et al . [ reference ][ reference ] use convolutions instead of deconvolutions in the decoder network . In contrast , our approach preserves high - resolution information throughout the entire network by keeping a separate high - resolution processing stream . Many approaches apply smoothing operations to the output of a CNN in order to obtain more consistent predictions . Most commonly , conditional random fields ( CRFs ) [ reference ] are applied on the network output [ reference ][ reference ][ reference ][ reference ][ reference ] . More recently , some papers approximate the mean - field inference of CRFs using specialized network architectures [ reference ][ reference ][ reference ] . Other approaches to smoothing the network predictions include domain transform [ reference ][ reference ] and superpixel - based smoothing [ reference ][ reference ] . Our approach is able to swiftly combine high - and low - resolution information , resulting in already smooth output predictions . Experiments with additional CRF smoothing therefore did not result in significant performance improvements . Network architectures . Since the success of the AlexNet architecture [ reference ] in the ImageNet Large - Scale Visual Classification Challenge ( ILSVRC ) [ reference ] , the vision community has seen several milestones with respect to CNN architectures . The network depth has been constantly increased , first with the popular VGG net [ reference ] , then by using batch normalization with GoogleNet [ reference ] . Lately , many computer vision applications have adopted the ResNet architecture [ reference ] , which often leads to signification performance boosts compared to earlier network architectures . All of these developments show how important a proper architecture is . However , so far most of these networks have been specifically tailored towards the task of classification , in many cases including a pre - training step on ILSVRC . As a result , some of their design choices may contribute to a suboptimal performance when performing pixel - to - pixel tasks such as semantic segmentation . In contrast , our proposed architecture has been specifically designed for segmentation tasks and reaches competitive performance on the Cityscapes dataset without requiring ILSVRC pre - training . section : Network Architectures for Segmentation Feed - Forward Networks . Until recently , the majority of feedforward networks , such as the VGG - variants [ reference ] , were composed of a linear sequence of layers . Each layer in such a network computes a function F and the output x n of the n - th layer is computed as where W n are the parameters of the layer ( see 2a ) . We refer to this class of network architectures as traditional feedforward networks . Residual Networks ( ResNets ) . He et al . observed that deepening traditional feedforward networks often results in an increased training loss [ reference ] . In theory , however , the training loss of a shallow network should be an upper bound on the training loss of a corresponding deep network . This is due to the fact that increasing the depth by adding layers strictly increases the expressive power of the model . A deep network can express all functions that the original shallow network can express by using identity mappings for the added layers . Hence a deep network should perform at least as well as the shallower model on the training data . The violation of this principle implied that current training algorithms have difficulties optimizing very deep traditional feedforward networks . He et al . proposed residual networks ( ResNets ) that exhibit significantly improved training characteristics , allowing network depths that were previously unattainable . A ResNet is composed of a sequence of residual units ( RUs ) . As depicted in Figure 2b , the output x n of the n - th RU in a ResNet is computed as where F ( x n−1 ; W n ) is the residual , which is parametrized by W n . Thus , instead of computing the output x n directly , F only computes a residual that is added to the input x n−1 . One commonly refers to this design as skip connection , because there is a connection from the input x n−1 to the output x n that skips the actual computation F. It has been empirically observed that ResNets have superior training properties over traditional feedforward networks . This can be explained by an improved gradient flow within the network . In oder to understand this , consider the n - th and m - th residual units in a ResNet where m > n ( i.e. , the m - th unit is closer to the output layer of the network ) . By applying the recursion ( 2 ) several times , He et al . showed in [ reference ] that the output of the m - th residual unit admits a representation of the form Furthermore , if l is the loss that is used to train the network , we can use the chain rule of calculus and express the derivative of the loss l with respect to the output x n of the n - th RU as Thus , we find We see that the weight updates depend on two sources of information , . While the amount of information that is contained in the latter may depend crucially on the depth n , the former allows a gradient flow that is independent of the depth . Hence , gradients can flow unhindered from the deeper unit to the shallower unit . This makes training even extremely deep ResNets possible . section : Full - Resolution Residual Networks ( FRRNs ) . In this paper , we unify the two above - mentioned principles of network design and propose full - resolution residual networks ( FRRNs ) that exhibit the same superior training properties as ResNets but have two processing streams . The features on one stream , the residual stream , are computed by adding successive residuals , while the features on the other stream , the pooling stream , are the direct result of a sequence of convolution and pooling operations applied to the input . Our design is motivated by the need to have networks that can jointly compute good high - level features for recognition and good low - level features for localization . Regardless of the specific network design , obtaining good highlevel features requires a sequence of pooling operations . The pooling operations reduce the size of the feature maps and increase the network 's receptive field , as well as its robustness against small translations in the image . While this is crucial to obtaining robust high - level features , networks that employ a deep pooling hierarchy have difficulties tracking low - level features , such as edges and boundaries , in deeper layers . This makes them good at recognizing the elements in a scene but bad at localizing them to pixel accuracy . On the other hand , a network that does not employ any pooling operations behaves the opposite way . It is good at localizing object boundaries , but performs poorly at recognizing the actual objects . By using the two processing streams together , we are able to compute both kinds of features simultaneously . While the residual stream of an FRRN computes successive residuals at the full image resolution , allowing low level features to propagate effortlessly through the network , the pooling stream undergoes a sequence of pooling and unpooling operations resulting in good high - level features . Figure 1 visualizes the concept of having two distinct processing streams . An FRRN is composed of a sequence of full - resolution residual units ( FRRUs ) . Each FRRU has two inputs and two outputs , because it simultaneously operates on both streams . Figure 2c shows the structure of an FRRU . Let z n−1 be the residual input to the n - th FRRU and let y n−1 be its pooling input . Then the outputs are computed as where W n are the parameters of the functions G and H , respectively . If G ≡ 0 , then an FRRU corresponds to an RU since it disregards the pooling input y n , and the network effectively becomes an ordinary ResNet . On the other hand , if H ≡ 0 , then the output of an FRRU only depends on its input via the function G. Hence , no residuals are computed and we obtain a traditional feedforward network . By carefully constructing G and H , we can combine the two network principles . In order to show that FRRNs have similar training characteristics as ResNets , we adapt the analysis presented in [ reference ] to our case . Using the same recursive argument as before , we find that for m > n , z m has the representation We can then express the derivative of the loss l with respect to the weights W n as Hence , the weight updates depend on three sources of information . Analogous to the analysis of ResNets , the two sources ∂H ( yi , zi;Wi + 1 ) ∂zn depend crucially on the depth n , while the term ∂l ∂zm is independent of the depth . Thus , we achieve a depth - independent gradient flow for all parameters that are used by the residual function H. If we use some of these weights in order to compute the output of G , all weights of the unit benefit from the improved gradient flow . This is most easily achieved by reusing the output of G in order to compute H. However , we note that other designs are possible . Figure 3 shows our proposed FRRU design . The unit first concatenates the two incoming streams by using a pooling layer in order to reduce the size of the residual stream . Then the concatenated features are fed through two convolution units . Each convolution unit consists of a 3 × 3 convolution layer followed by a batch normalization layer [ reference ] and a ReLU activation function . The result of the second convolution unit is used in two ways . First , it forms the pooling stream input of the next FRRU in the network and second it is the basis for the computed residual . To this end , we first adjust the number of feature channels using a 1 × 1 convolution and then upscale the spatial dimensions using an unpooling layer . Because the features might have to be upscaled significantly ( e.g. , by a factor of 16 ) , we found that simply upscaling by repeating the entries along the spatial dimensions performed superior to bilinear interpolation . In Figure 3 , the inner red box corresponds to the function G while the outer blue box corresponds to the function H. We can see that the output of G is used in order to compute H , because the red box is entirely contained within the blue box . As shown above , this design choice results in superior gradient flow properties for all weights of the unit . Table 1 shows the two network architectures that we used in order to assess our approach 's segmentation performance . The proposed architectures are based on several principles employed by other authors . We follow Noh et al . [ reference ] and use an encoder / decoder formulation . In the encoder , we reduce the size of the pooling stream using max pooling operations . The pooled feature maps are then successively upscaled using bilinear interpolation in the decoder . Furthermore , similar to Simonyan and Zisserman [ reference ] , we define a number of base channels that we double after each pooling operation ( up to a certain upper limit ) . Instead of choosing 64 base channels as in VGG net , we use 48 channels in order to have a manageable number of trainable parameters . Depending on the input image resolution , we use FRRN A or FRRN B to keep the relative size of the receptive fields consistent . section : Training Procedure Following Wu et al . , we train our network by minimizing a bootstrapped cross - entropy loss [ reference ] . Let c be the number of classes , y 1 , ... , y N ∈ { 1 , ... , c } be the target class labels for the pixels 1 , ... , N , and let p i , j be the posterior class + Bias Softmax probability for class j and pixel i. Then , the bootstrapped cross - entropy loss over K pixels is defined as where 1 [ x ] = 1 iff x is true and t k ∈ R is chosen such that |{i ∈ { 1 , ... , N } : p i , yi < t k } | = K. The threshold parameter t k can easily be determined by sorting the predicted log probabilities and choosing the K + 1 - th one as threshold . Figure 4 visualizes the concept . Depending on the number of pixels K that we consider , we select misclassified pixels or pixels where we predict the correct label with a small probability . We minimize the loss using ADAM [ reference ] . Because each FRRU processes features at the full image resolution , training a full - resolution residual network is very memory intensive . Recall that in order for the backpropagation algorithm [ reference ] to work , the entire forward pass has to be stored in memory . If the memory required to store the forward pass for a given network exceeds the available GPU memory , we can no longer use the standard backpropagation algorithm . In order to alleviate this problem , Figure 4 . Pixels used by the bootstrapped cross - entropy loss for varying values of K. The images and ground truth annotations originate from the twice - subsampled Cityscapes validation set [ reference ] . Pixels that are labeled void are not considered for the bootstrapping process . we partition the computation graph into several subsequent blocks by manually placing cut points in the graph . We then compute the derivatives for each block individually . To this end , we perform one ( partial ) forward pass per block and only store the feature maps for the block whose derivatives are computed given the derivative of the subsequent block . This simple scheme allows us to manually control a spacetime trade - off . The idea of recomputing some intermediate results on demand is also used in [ reference ] and [ reference ] . Note that these memory limitations only apply during training . During testing , there is no need to store results of each operation in the network and our architecture 's memory footprint is comparable to that of a ResNet encoder / decoder architecture . We will make code for the gradient computation for arbitrary networks publicly available in Theano / Lasagne . In order to reduce overfitting , we used two methods of data augmentation : translation augmentation and gamma augmentation . The former method randomly translates an image and its annotations . In order to keep consistent image dimensions , we have to pad the translated images and annotations . To this end , we use reflection padding on the image and constant padding with void labels on the annotations . Our second method of data augmentation is gamma augmentation . We use a slightly modified gamma augmentation method detailed in Appendix A. section : Experimental Evaluation We evaluate our approach on the recently released Cityscapes benchmark [ reference ] containing images recorded in 50 different cities . This benchmark provides 5 , 000 images with high - quality annotations split up into a training , validation , and test set ( 2 , 975 , 500 , and 1 , 525 images , respectively ) . The dense pixel annotations span 30 classes frequently occurring in urban street scenes , out of which 19 are used for actual training and evaluation . Annotations for the test set remain private and comparison to other methods is performed via a dedicated evaluation server . We report the results of our FRRNs for two settings : FRRN A trained on quarter - resolution ( 256 × 512 ) Cityscapes images ; and FRRN B trained on half - resolution ( 512 × 1024 ) images . We then upsample our predictions using bilinear interpolation in order to report scores at the full image resolution of 1024 × 2048 pixels . Directly training at the full Cityscapes resolution turned out to be too memory intensive with our current design . However , as our experimental results will show , even when trained only on halfresolution images , our FRRN B 's results are competitive with the best published methods trained on full - resolution data . Unless specified otherwise , the reported results are based on the Cityscapes test set . Qualitative results are shown in Figure 7 , in Appendix C , and in our result video 2 . section : Residual Network Baseline Our network architecture can be described as a ResNet [ reference ] encoder / decoder architecture , where the residuals remain at the full input resolution throughout the network . A natural baseline is thus a traditional ResNet encoder / decoder architecture with long - range skip connections [ reference ][ reference ] . In fact , such an architecture resembles a single deep hourglass module in the stacked hourglass network architecture [ reference ] . This baseline differs from our proposed architecture in two important ways : While the feature maps on our residual stream are processed by each FRRU , the feature maps on the long - range skip connections are not processed by intermediate layers . Furthermore , long - range skip connections are scale dependent , meaning that features at one scale travel over a different skip connection than features at another scale . This is in contrast to our network design , where the residual stream can carry upscaled features from several pooling stages simultaneously . In order to illustrate the benefits of our approach over the natural baseline , we converted the architecture FRRN A ( Table 1a ) to a ResNet as follows : We first replaced all FRRUs by RUs and then added skip connections that connect the input of each pooling layer to the output of the corresponding unpooling layer . The resulting ResNet has slightly fewer parameters than the original FRRN ( 16.7 × 10 6 vs. 17.7 × 10 6 ) . This is due to the fact that RUs lack the 1 × 1 convolutions that connect the pooling to the residual Table 2 . IoU scores from the cityscapes test set . We highlight the best published baselines for the different sampling rates . ( Additional anonymous submissions exist as concurrent work . ) Bold numbers represent the best , italic numbers the second best score for a class . We also indicate the subsampling factor used on the input images , whether additional coarsely annotated data was used , and whether the model was initialized with pre - trained weights . stream . section : Method We train both networks on the quarter - resolution Cityscapes dataset for 45 , 000 iterations at a batch size of 3 . We use a learning rate of 10 −3 for the first 35 , 000 iterations and then reduce it to 10 −4 for the following 10 , 000 iterations . Both networks converged within these iterations . The FRRN A resulted in a validation set mean IoU score of 65.7 % while the ResNet baseline only achieved 62.8 % , showing a significant advantage of our FRRNs . Training FRRN B is performed in a similar fashion . Detailed training curves are shown in Appendix B. section : Quantitative Evaluation Overview In Table 2 we compare our method to the best ( published ) performers on the Cityscapes leader board , namely LRR [ reference ] , Adelaide [ reference ] , and Dilation [ reference ] . Note that our network performs on par with the very complex and well engineered system by [ reference ] . Among the top performers on Cityscapes , only ENet refrain from using a pre - trained network . However , they design their network for real time performance and thus do not obtain top scores . To the best of our knowledge , we are the first to show that it is possible to obtain state - of - the - art results even without pre - training . This gives credibility to our claim that network architectures can have a crucial effect on a system 's overall performance . Subsampling Factor . An interesting observation that we made on the Cityscapes test set is a correlation between the subsampling factor and the test performance . This correlation can be seen in Figure 5 where we show the scores of several approaches currently listed on the leader board against their respective subsampling factors . Unsurprisingly , most of the best performers operate on the fullresolution input images . Throughout our experiments , we consistently outperformed other approaches who trained on Mean IoU Score ( % ) section : Subsampling factor Published Unpublished LRR [ reference ] Adelaide [ reference ] Dilation [ reference ] ENet [ reference ] SegNet [ reference ] DeepLab [ reference ] FRRN A / B Figure 5 . Comparison of the mean IoU scores of all approaches on the leader board of the Cityscapes segmentation benchmark based on the subsampling factor of the images that they were trained on . Dilation [ reference ] LRR [ reference ] FRRN B Figure 6 . The trimap evaluation on the validation set . The solid lines show the mean IoU score of our approach and two top performing methods that released their code . The dashed lines show the mean IoU score when using the 7 Cityscapes category labels for the same methods . the same image resolutions . Even though we only train on half - resolution images , Figure 5 clearly shows we can match the current published state - of - the - art ( LRR [ reference ] Figure 7 . Qualitative comparison on the Cityscapes validation set . Interesting cases are the fence in the first row , the truck in the second row , or the street light poles in the last row . An interesting failure case is shown in the third row : all methods struggle to find the correct sidewalk boundary , however our network makes a clean and reasonable prediction . section : Boundary Adherence Due to several pooling operations ( and subsequent upsampling ) in many of today 's FCN architectures , boundaries are often overly smooth , resulting in lost details and edge - bleeding . This leads to suboptimal scores , but it also makes the output of a semantic segmentation approach harder to use without further post - processing . Since inaccurate boundaries are often not apparent from the standard evaluation metric scores , a typical approach is a trimap evaluation in order to quantify detailed boundary adherence [ reference ][ reference ][ reference ] . During trimap evaluation , all predictions are ignored if they do not fall within a certain radius r of a ground truth label boundary . Figure 6 visualizes our trimap evaluation performed on the validation set for varying trimap widths r between 1 and 80 pixels . We compare to LRR [ reference ] and Dilation [ reference ] , who made code and pre - trained models available . We see that our approach outperforms the competition consistently for all radii r. Furthermore , it shall be noted that the method of [ reference ] is based on an architecture specifically designed for clean boundaries . Our method achieves better boundary adherence , both numerically and qualitatively ( see Figure 7 ) , with a much simpler architecture and without ImageNet pre - training . Often one can boost both the numerical score and the boundary adherence by using a fully connected CRF as post - processing step . We tried to apply a fully connected CRF with Gaussian kernel , as introduced by Krähenbühl and Kolton [ reference ] . We used the standard appearance and smoothness kernels and tuned parameters on the validation set by running several thousand Hyperopt iterations [ reference ] . Surprisingly the color standard deviation for the appearance kernel tended towards very small values , while the weight did not go to zero . This indicates that the appearance kernel would only smooth labels across pixels with very similar colors . Nevertheless , with the best parameters we only obtained an IoU boost of ∼ 0.5 % on the validation set . Given the high computation time we decided against any post - processing steps . section : Conclusion In this paper we propose a novel network architecture for semantic segmentation in street scenes . Our architecture is clean , does not require additional post - processing , can be trained from scratch , shows superior boundary adherence , and reaches state - of - the - art results on the Cityscapes benchmark . We will provide code and all trained models . Since we do not incorporate design choices specifically tailored towards semantic segmentation , we believe that our architecture will also be applicable to other tasks such as stereo or optical flow where predictions are performed per pixel . Our goal is to find γ such that E U [ U ] = 0.5 . The key idea to solving this problem is to look at the deviation of U from 0.5 . Let Z be this deviation . Then ( 11 ) is equivalent to Hence , without solving for the implicitly defined variable U explicitly , we found a transformation of a zero - mean random variable Z such that γ has the desired properties . Because Z was defined to be the offset from 0.5 and U ∈ [ 0 , 1 ] , it follows Z ∈ [ −0.5 , 0.5 ] . We are free to choose any distribution such that Z has zero mean and falls into the range [ −0.5 , 0.5 ] . For simplicity reasons , we choose Z to be uniformly distributed over [ −a , a ] where a ∈ [ 0 , 0.5 ] determines the strength of the augmentation . Figure 8b illustrates the obvious bias reduction . section : B. Baseline Evaluation In Section 5.2 of the main paper , we describe the setting of our baseline method ( Residual Network Baseline ) and compare it to our FRRN A network . To emphasize on a proper training procedure of both baselines , Figure 9 shows the mean IoU score on the validation set over time . We can see that our model outperforms the baseline with a significant margin and both methods are trained until convergence . Figure 10 shows and compares addtional output labelings of our method . Please also consult our labeled video sequence [ reference ] to gain a better sense of the quality of our method . We all know Latex is a pain . section : C. Qualitative Results section : Image Ground Truth Ours LRR [ reference ] Figure 10 . Additional qualitative results on the Cityscapes validation set . We omit the comparison to Dilation [ reference ] in order to show bigger images here . section : section : Appendix section : A. Gamma Augmentation Gamma augmentation is an augmentation method that varies the image contrast and brightness . Assume the intensity values of an image are scaled to the unit interval [ 0 , 1 ] . Then gamma augmentation applies the intensity transformation x → x γ for a randomly sampled augmentation parameter γ > 0 . However , sampling the augmentation parameter γ is not trivial . Naively drawing samples from a uniform or truncated Gaussian distribution with a mean of 1 results in a noticeable bias ( Figure 8a ) . In order to reduce the bias , we deduce a novel sampling schema for γ . Let U be a random variable that is implicitly defined as the solution to the fixed - point problem section :\"]}\n",
      "| {'label': 'named entity recognition', 'text': ['document : Evaluating the Utility of Hand - crafted Features in Sequence Labelling Conventional wisdom is that hand - crafted features are redundant for deep learning models , as they already learn adequate representations of text automatically from corpora . In this work , we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach , incorporating a feature auto - encoder loss component . We evaluate on the task of named entity recognition ( NER ) , where we show that including manual features for part - of - speech , word shapes and gazetteers can improve the performance of a neural CRF model . We obtain a of 91.89 for the CoNLL - 2003 English shared task , which significantly outperforms a collection of highly competitive baseline models . We also present an ablation study showing the importance of auto - encoding , over using features as either inputs or outputs alone , and moreover , show including the autoencoder components reduces training requirements to 60 % , while retaining the same predictive accuracy . section : Introduction Deep neural networks have been proven to be a powerful framework for natural language processing , and have demonstrated strong performance on a number of challenging tasks , ranging from machine translation , to text categorisation . Not only do such deep models outperform traditional machine learning methods , they also come with the benefit of not requiring difficult feature engineering . For instance , both lample2016neural and ma2016end propose end - to - end models for sequence labelling task and achieve state - of - the - art results . Orthogonal to the advances in deep learning is the effort spent on feature engineering . A representative example is the task of named entity recognition ( NER ) , one that requires both lexical and syntactic knowledge , where , until recently , most models heavily rely on statistical sequential labelling models taking in manually engineered features . Typical features include POS and chunk tags , prefixes and suffixes , and external gazetteers , all of which represent years of accumulated knowledge in the field of computational linguistics . The work of collobert2011natural started the trend of feature engineering - free modelling by learning internal representations of compositional components of text ( e.g. , word embeddings ) . Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real - valued vectors trained on large unannotated corpora . Enabled by the powerful representational capacity of such embeddings and neural networks , feature engineering has largely been replaced with taking off - the - shelf pre - trained word embeddings as input , thereby making models fully end - to - end and the research focus has shifted to neural network architecture engineering . More recently , there has been increasing recognition of the utility of linguistic features where such features are integrated to improve model performance . Inspired by this , taking NER as a case study , we investigate the utility of hand - crafted features in deep learning models , challenging conventional wisdom in an attempt to refute the utility of manually - engineered features . Of particular interest to this paper is the work by ma2016end where they introduce a strong end - to - end model combining a bi - directional Long Short - Term Memory ( Bi - LSTM ) network with Convolutional Neural Network ( CNN ) character encoding in a Conditional Random Field ( CRF ) . Their model is highly capable of capturing not only word - but also character - level features . We extend this model by integrating an auto - encoder loss , allowing the model to take hand - crafted features as input and re - construct them as output , and show that , even with such a highly competitive model , incorporating linguistic features is still beneficial . Perhaps the closest to this study is the works by Ammar + :2014 and Zhang + :2017 , who show how CRFs can be framed as auto - encoders in unsupervised or semi - supervised settings . With our proposed model , we achieve strong performance on the CoNLL 2003 English NER shared task with an of , significantly outperforming an array of competitive baselines . We conduct an ablation study to better understand the impacts of each manually - crafted feature . Finally , we further provide an in - depth analysis of model performance when trained with varying amount of data and show that the proposed model is highly competent with only 60 % of the training set . section : Methodology In this section , we first outline the model architecture , then the manually crafted features , and finally how they are incorporated into the model . subsection : Model Architecture We build on a highly competitive sequence labelling model , namely Bi - LSTM - CNN - CRF , first introduced by ma2016end . Given an input sequence of of length , the model is capable of tagging each input with a predicted label , resulting in a sequence of closely matching the gold label sequence . Here , we extend the model by incorporating an auto - encoder loss taking hand - crafted features as in / output , thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance . Specifically , our model , referred to as Neural - CRF + AE , consists of four major components : ( 1 ) a character - level CNN ( char - CNN ) ; ( 2 ) a word - level bi - directional LSTM ( Bi - LSTM ) ; ( 3 ) a conditional random field ( CRF ) ; and ( 4 ) an auto - encoder auxiliary loss . An illustration of the model architecture is presented in Figure [ reference ] . paragraph : Char - CNN . Previous studies have demonstrated that CNNs are highly capable of capturing character - level features . Here , our character - level CNN is similar to that used in ma2016end but differs in that we use a ReLU activation . paragraph : Bi - LSTM . We use a Bi - LSTM to learn contextual information of a sequence of words . As inputs to the Bi - LSTM , we first concatenate the pre - trained embedding of each word with its character - level representation ( the output of the char - CNN ) and a vector of manually crafted features ( described in Section [ reference ] ) : where denotes concatenation . The outputs of the forward and backward pass of the Bi - LSTM is then concatenated to form the output of the Bi - LSTM , where dropout is also applied . paragraph : CRF . For sequence labelling tasks , it is intuitive and beneficial to utilise information carried between neighbouring labels to predict the best sequence of labels for a given sentence . Therefore , we employ a conditional random field layer taking as input the output of the Bi - LSTM . Training is carried out by maximising the log probability of the gold sequence : while decoding can be efficiently performed with the Viterbi algorithm . paragraph : Auto - encoder loss . Alongside sequence labelling as the primary task , we also deploy , as auxiliary tasks , three auto - encoders for reconstructing the hand - engineered feature vectors . To this end , we add multiple independent fully - connected dense layers , all taking as input the Bi - LSTM output with each responsible for reconstructing a particular type of feature : where is the sigmoid activation function , denotes the type of feature , and is a trainable parameter matrix . More formally , we define the auto - encoder loss as : Model training . Training is carried out by optimising the joint loss : where , in addition to , we also add the auto - encoder loss , weighted by . In all our experiments , we set to for all s. subsection : Hand - crafted Features We consider three categories of widely used features : ( 1 ) POS tags ; ( 2 ) word shape ; and ( 3 ) gazetteers and present an example in Table [ reference ] . While POS tags carry syntactic information regarding sentence structure , the word shape feature focuses on a more fine - grained level , encoding character - level knowledge to complement the loss of information caused by embedding lookup , such as capitalisation . Both features are based on the implementation of spaCy . For the gazetteer feature , we focus on PERSON and LOCATION and compile a list for each . The PERSON gazetteer is collected from U.S. census 2000 , U.S. census 2010 and DBpedia whereas GeoNames is the main source for LOCATION , taking in both official and alternative names . All the tokens on both lists are then filtered to exclude frequently occurring common words . Each category is converted into a one - hot sparse feature vector and then concatenated to form a multi - hot vector for the - th word . In addition , we also experimented with including the label of the incoming dependency edge to each word as a feature , but observed performance deterioration on the development set . While we still study and analyse the impacts of this feature in Table [ reference ] and Section [ reference ] , it is excluded from our model configuration ( not considered as part of unless indicated otherwise ) . section : Experiments In this section , we present our experimental setup and results for name entity recognition over the CoNLL 2003 English NER shared task dataset . subsection : Experimental Setup paragraph : Dataset . We use the CoNLL 2003 NER shared task dataset , consisting of 14 , 041 / 3 , 250 / 3 , 453 sentences in the training / development / test set respectively , all extracted from Reuters news articles during the period from 1996 to 1997 . The dataset is annotated with four categories of name entities : PERSON , LOCATION , ORGANIZATION and MISC . We use the IOBES tagging scheme , as previous study have shown that this scheme provides a modest improvement to the model performance . paragraph : Model configuration . Following the work of ma2016end , we initialise word embeddings with GloVe ( - dimensional , trained on a 6B - token corpus ) . Character embeddings are - dimensional and randomly initialised with a uniform distribution in the range . Parameters are optimised with stochastic gradient descent ( SGD ) with an initial learning rate of and momentum of . Exponential learning rate decay is applied every 5 epochs with a factor of . To reduce the impact of exploding gradients , we employ gradient clipping at . We train our models on a single GeForce GTX TITAN X GPU . With the above hyper - parameter setting , training takes approximately hours for a full run of epochs . paragraph : Evaluation . We measure model performance with the official CoNLL evaluation script and report span - level named entity F - score on the test set using early stopping based on the performance on the validation set . We report average F - scores and standard deviation over 5 runs for our model . paragraph : Baseline . In addition to reporting a number of prior results of competitive baseline models , as listed in Table [ reference ] , we also re - implement the Bi - LSTM - CNN - CRF model by ma2016end ( referred to as Neural - CRF in Table [ reference ] ) and report its average performance . subsection : Results The experimental results are presented in Table [ reference ] . Observe that Neural - CRF + AE , trained either on the training set only or with the addition of the development set , achieves substantial improvements in F - score in both settings , superior to all but one of the benchmark models , highlighting the utility of hand - crafted features incorporated with the proposed auto - encoder loss . Compared against the Neural - CRF , a very strong model in itself , our model significantly improves performance , showing the positive impact of our technique for exploiting manually - engineered features . Although Peters + :2018 report a higher F - score using their ELMo embedding technique , our approach here is orthogonal , and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model . paragraph : Ablation Study To gain a better understanding of the impacts of each feature , we perform an ablation study and present the results in Table [ reference ] . We observe performance degradation when eliminating POS , word shape and gazetteer features , showing that each feature contributes to NER performance beyond what is learned through deep learning alone . Interestingly , the contribution of gazetteers is much less than that of the other features , which is likely due to the noise introduced in the matching process , with many incorrectly identified false positives . Including features based on dependency tags into our model decreases the performance slightly . This might be a result of our simple implementation ( as illustrated in Table [ reference ] ) , which does not include dependency direction , nor parent - child relationships . Next , we investigate the impact of different means of incorporating manually - engineered features into the model . To this end , we experiment with three configurations with features as : ( 1 ) input only ; ( 2 ) output only ( equivalent to multi - task learning ) ; and ( 3 ) both input and output ( Neural - CRF + AE ) and present the results in Table [ reference ] . Simply using features as either input or output only improves model performance slightly , but insignificantly so . It is only when features are incorporated with the proposed auto - encoder loss do we observe a significant performance boost . paragraph : Training Requirements every node= [ font= ] [ xlabel = Fraction of training data ( % ) , ylabel = F1 score , xmin=7 , xmax=103 , ymin=84 , ymax=93 , height=4.5 cm , ymajorgrids = true , grid style = dashed , width=0.48legend pos = south east ] [ color = red , error bars / .cd , y dir = both , y explicit ] coordinates ( 10 , 85.19 )+ = ( 10 , 0.34 )- = ( 10 , 0.34 ) ( 20 , 88.27 )+ = ( 20 , 0.29 )- = ( 20 , 0.29 ) ( 30 , 89.35 )+ = ( 30 , 0.28 )- = ( 30 , 0.28 ) ( 40 , 89.70 )+ = ( 40 , 0.30 )- = ( 40 , 0.29 ) ( 50 , 90.55 )+ = ( 50 , 0.27 )- = ( 50 , 0.27 ) ( 60 , 91.09 )+ = ( 60 , 0.26 )- = ( 60 , 0.26 ) ( 70 , 91.26 )+ = ( 70 , 0.25 )- = ( 70 , 0.25 ) ( 80 , 91.40 )+ = ( 80 , 0.24 )- = ( 80 , 0.24 ) ( 90 , 91.64 )+ = ( 90 , 0.25 )- = ( 90 , 0.25 ) ( 100 , 91.89 )+ = ( 100 , 0.23 )- = ( 100 , 0.23 ) ; dashed ] coordinates ( 10 , 91.06 )( 100 , 91.06 ) ; Neural systems typically require a large amount of annotated data . Here we measure the impact of training with varying amount of annotated data , as shown in Figure [ reference ] . Wtih the proposed model architecture , the amount of labelled training data can be drastically reduced : our model , achieves comparable performance against the baseline Neural - CRF , with as little as 60 % of the training data . Moreover , as we increase the amount of training text , the performance of Neural - CRF + AE continues to improve . paragraph : Hyperparameters every node= [ font= ] [ xmode = log , xlabel = λi , ylabel = F1 score , xmin=0 , xmax=90 , ymin=91.3 , ymax=92 , height=5.5 cm , ymajorgrids = true , grid style = dashed , width=0.48legend style = at= ( 0.87 , 0 ), anchor = south east ] [ color = red , error bars / .cd , y dir = both , y explicit ] coordinates ( 1e - 8 , 91.39 )+ = ( 1e - 8 , 0.0484 )- = ( 1e - 8 , 0.0484 ) ( 1e - 7 , 91.43 )+ = ( 1e - 7 , 0.0361 )- = ( 1e - 7 , 0.0361 ) ( 1e - 6 , 91.50 )+ = ( 1e - 6 , 0.04 )- = ( 1e - 6 , 0.04 ) ( 1e - 5 , 91.56 )+ = ( 1e - 5 , 0.0529 )- = ( 1e - 5 , 0.0529 ) ( 1e - 4 , 91.63 )+ = ( 1e - 4 , 0.0576 )- = ( 1e - 4 , 0.0576 ) ( 1e - 3 , 91.62 )+ = ( 1e - 3 , 0.0484 )- = ( 1e - 3 , 0.0484 ) ( 1e - 2 , 91.67 )+ = ( 1e - 2 , 0.0361 )- = ( 1e - 2 , 0.0361 ) ( 1e - 1 , 91.74 )+ = ( 1e - 1 , 0.04 )- = ( 1e - 1 , 0.04 ) ( 1 , 91.89 )+ = ( 1 , 0.0529 )- = ( 1 , 0.0529 ) ( 10 , 91.56 )+ = ( 10 , 0.0289 )- = ( 10 , 0.0289 ) ; tagging [ color = blue , error bars / .cd , y dir = both , y explicit ] coordinates ( 1e - 8 , 91.41 )+ = ( 1e - 8 , 0.0289 )- = ( 1e - 8 , 0.0289 ) ( 1e - 7 , 91.45 )+ = ( 1e - 7 , 0.0361 )- = ( 1e - 7 , 0.0361 ) ( 1e - 6 , 91.52 )+ = ( 1e - 6 , 0.0324 )- = ( 1e - 6 , 0.0324 ) ( 1e - 5 , 91.57 )+ = ( 1e - 5 , 0.04 )- = ( 1e - 5 , 0.04 ) ( 1e - 4 , 91.66 )+ = ( 1e - 4 , 0.0441 )- = ( 1e - 4 , 0.0441 ) ( 1e - 3 , 91.68 )+ = ( 1e - 3 , 0.0361 )- = ( 1e - 3 , 0.0361 ) ( 1e - 2 , 91.70 )+ = ( 1e - 2 , 0.04 )- = ( 1e - 2 , 0.04 ) ( 1e - 1 , 91.78 )+ = ( 1e - 1 , 0.0484 )- = ( 1e - 1 , 0.0484 ) ( 1 , 91.89 )+ = ( 1 , 0.0529 )- = ( 1 , 0.0529 ) ( 10 , 91.50 )+ = ( 10 , 0.0324 )- = ( 10 , 0.0324 ) ; Shape [ color = green , error bars / .cd , y dir = both , y explicit ] coordinates ( 1e - 8 , 91.72 )+ = ( 1e - 8 , 0.0225 )- = ( 1e - 8 , 0.0225 ) ( 1e - 7 , 91.78 )+ = ( 1e - 7 , 0.0256 )- = ( 1e - 7 , 0.0256 ) ( 1e - 6 , 91.76 )+ = ( 1e - 6 , 0.0324 )- = ( 1e - 6 , 0.0324 ) ( 1e - 5 , 91.79 )+ = ( 1e - 5 , 0.0484 )- = ( 1e - 5 , 0.0484 ) ( 1e - 4 , 91.80 )+ = ( 1e - 4 , 0.0576 )- = ( 1e - 4 , 0.0576 ) ( 1e - 3 , 91.83 )+ = ( 1e - 3 , 0.0484 )- = ( 1e - 3 , 0.0484 ) ( 1e - 2 , 91.85 )+ = ( 1e - 2 , 0.0361 )- = ( 1e - 2 , 0.0361 ) ( 1e - 1 , 91.82 )+ = ( 1e - 1 , 0.04 )- = ( 1e - 1 , 0.04 ) ( 1 , 91.89 )+ = ( 1 , 0.0529 )- = ( 1 , 0.0529 ) ( 10 , 91.56 )+ = ( 10 , 0.0289 )- = ( 10 , 0.0289 ) ; Three extra hyperparameters are introduced into our model , controlling the weight of the autoencoder loss relative to the CRF loss , for each feature type . Figure [ reference ] shows the effect of each hyperparameter on test performance . Observe that setting gives strong performance , and that the impact of the gazetteer is less marked than the other two feature types . While increasing is mostly beneficial , performance drops if the are overly large , that is , the auto - encoder loss overwhelms the main prediction task . section : Conclusion In this paper , we set out to investigate the utility of hand - crafted features . To this end , we have presented a hybrid neural architecture to validate this hypothesis extending a Bi - LSTM - CNN - CRF by incorporating an auto - encoder loss to take manual features as input and then reconstruct them . On the task of named entity recognition , we show significant improvements over a collection of competitive baselines , verifying the value of such features . Lastly , the method presented in this work can also be easily applied to other tasks and models , where hand - engineered features provide key insights about the data . bibliography : References']}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'papers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/scratch/211679273.tmpdir/ipykernel_97269/3347258198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_of_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mapping'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'papers'"
     ]
    }
   ],
   "source": [
    "print('#', task_name, k, seed, beam)\n",
    "dataset_path = os.path.join(data_dir, task_name)#, \"{}-{}\".format(k, seed))\n",
    "dataset = load_dataset(task_name, dataset_path)\n",
    "print('|', 'dataset examples')\n",
    "print('|', dataset[0])\n",
    "print('|', dataset[-1])\n",
    "print()\n",
    "\n",
    "# Manual label word mappings\n",
    "map_of_mapping = {\n",
    "    'SST-2': {'0':'terrible','1':'great'},\n",
    "    'sst-5': {0:'terrible',1:'bad',2:'okay',3:'good',4:'great'},\n",
    "    'mr': {0:'terrible',1:'great'},\n",
    "    'cr': {0:'terrible',1:'great'},\n",
    "    'subj': {0:'subjective',1:'objective'},\n",
    "    'trec': {0:'Description',1:'Entity',2:'Expression',3:'Human',4:'Location',5:'Number'},\n",
    "    'mpqa': {0:'terrible',1:'great'},\n",
    "    'CoLA': {'0':'incorrect','1':'correct'},\n",
    "    'MRPC': {'0':'No','1':'Yes'},\n",
    "    'QQP': {'0':'No','1':'Yes'},\n",
    "    'STS-B': {'0':'No','1':'Yes'},\n",
    "    'MNLI': {'contradiction':'No','entailment':'Yes','neutral':'Maybe'},\n",
    "    'SNLI': {'contradiction':'No','entailment':'Yes','neutral':'Maybe'},\n",
    "    'QNLI': {'not_entailment':'No','entailment':'Yes'},\n",
    "    'RTE': {'not_entailment':'No','entailment':'Yes'}\n",
    "}\n",
    "\n",
    "mapping = map_of_mapping[task_name]\n",
    "print('|', 'mapping')\n",
    "print('|', mapping)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, task_name), exist_ok=True)\n",
    "f = open(os.path.join(output_dir, task_name, \"{}-{}.txt\".format(k, seed)), 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35b06377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### example #######\n",
      "Full - Resolution Residual Networks for Semantic Segmentation in Street Scenes section : Abstract Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state - of - the - art approaches in semantic image segmentation rely on pretrained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance ( i.e., what is visible? ), they lack localization accuracy ( i.e., where precisely is something located? ). Therefore, additional processing steps have to be performed in order to obtain pixel - accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet - like architecture that exhibits strong localization and recognition performance. We combine multi - scale context with pixel - level accuracy by using two processing streams within our network : One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence\n",
      "Document : V - Net : Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation Convolutional Neural Networks ( CNNs ) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end - to - end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non - linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves\n",
      "Document : Bi - Directional Attention Flow for Machine Comprehension Machine comprehension ( MC ), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed - size vector, couple attentions temporally, and / or often form a uni - directional attention. In this paper we introduce the Bi - Directional Attention Flow ( BiDAF ) network, a multi - stage hierarchical process that represents the context at different levels of granularity and uses bi - directional attention flow mechanism to obtain a query - aware context representation without early summarization. Our experimental evaluations show that our model achieves the state - of - the - art results in Stanford Question Answering Dataset ( SQuAD ) and CNN / DailyMail cloze test. section : Introduction The tasks of machine comprehension ( MC ) and question answering ( QA\n",
      "####### example #######\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [24:30<00:00, 81.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### generated results #######\n",
      "--------------\n",
      "score: -10.811141967773438\n",
      "generated ids [32099, 6, 32098, 6, 32097]\n",
      "generated text <extra_id_0>,<extra_id_1>,<extra_id_2>\n",
      "--------------\n",
      "score: -10.851200103759766\n",
      "generated ids [32099, 6, 32098, 11, 32097]\n",
      "generated text <extra_id_0>,<extra_id_1>▁and<extra_id_2>\n",
      "--------------\n",
      "score: -10.907706260681152\n",
      "generated ids [32099, 11, 32098, 6, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>,<extra_id_2>\n",
      "--------------\n",
      "score: -11.312496185302734\n",
      "generated ids [32099, 11, 32098, 18, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>-<extra_id_2>\n",
      "--------------\n",
      "score: -11.403470039367676\n",
      "generated ids [32099, 11, 32098, 5, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>.<extra_id_2>\n",
      "--------------\n",
      "score: -11.557948112487793\n",
      "generated ids [32099, 11, 32098, 11, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>▁and<extra_id_2>\n",
      "--------------\n",
      "score: -11.57657527923584\n",
      "generated ids [32099, 11, 32098, 13, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>▁of<extra_id_2>\n",
      "--------------\n",
      "score: -11.585579872131348\n",
      "generated ids [32099, 11, 32098, 8, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>▁the<extra_id_2>\n",
      "--------------\n",
      "score: -11.714678764343262\n",
      "generated ids [32099, 16, 32098, 6, 32097]\n",
      "generated text <extra_id_0>▁in<extra_id_1>,<extra_id_2>\n",
      "--------------\n",
      "score: -11.718791961669922\n",
      "generated ids [32099, 6, 32098, 18, 32097]\n",
      "generated text <extra_id_0>,<extra_id_1>-<extra_id_2>\n",
      "--------------\n",
      "score: -11.796246528625488\n",
      "generated ids [32099, 18, 32098, 18, 32097]\n",
      "generated text <extra_id_0>-<extra_id_1>-<extra_id_2>\n",
      "--------------\n",
      "score: -11.818023681640625\n",
      "generated ids [32099, 18, 32098, 11, 32097]\n",
      "generated text <extra_id_0>-<extra_id_1>▁and<extra_id_2>\n",
      "--------------\n",
      "score: -11.841190338134766\n",
      "generated ids [32099, 6, 32098, 8, 32097]\n",
      "generated text <extra_id_0>,<extra_id_1>▁the<extra_id_2>\n",
      "--------------\n",
      "score: -11.880077362060547\n",
      "generated ids [32099, 16, 32098, 11, 32097]\n",
      "generated text <extra_id_0>▁in<extra_id_1>▁and<extra_id_2>\n",
      "--------------\n",
      "score: -11.900395393371582\n",
      "generated ids [32099, 8, 32098, 13, 32097]\n",
      "generated text <extra_id_0>▁the<extra_id_1>▁of<extra_id_2>\n",
      "--------------\n",
      "score: -11.928167343139648\n",
      "generated ids [32099, 11, 32098, 16, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>▁in<extra_id_2>\n",
      "--------------\n",
      "score: -11.949409484863281\n",
      "generated ids [32099, 11, 32098, 12, 32097]\n",
      "generated text <extra_id_0>▁and<extra_id_1>▁to<extra_id_2>\n",
      "--------------\n",
      "score: -11.963299751281738\n",
      "generated ids [32099, 5, 32098, 11, 32097]\n",
      "generated text <extra_id_0>.<extra_id_1>▁and<extra_id_2>\n",
      "--------------\n",
      "score: -11.997247695922852\n",
      "generated ids [32099, 12, 32098, 8, 32097]\n",
      "generated text <extra_id_0>▁to<extra_id_1>▁the<extra_id_2>\n",
      "--------------\n",
      "score: -12.085896492004395\n",
      "generated ids [32099, 6, 32098, 13, 32097]\n",
      "generated text <extra_id_0>,<extra_id_1>▁of<extra_id_2>\n",
      "--------------\n",
      "score: -29.430644989013672\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 331]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁data\n",
      "--------------\n",
      "score: -30.26736831665039\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 8, 331]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁the▁data\n",
      "--------------\n",
      "score: -30.412160873413086\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the\n",
      "--------------\n",
      "score: -30.744810104370117\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 24, 54, 36, 261, 12]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁that▁can▁be▁used▁to\n",
      "--------------\n",
      "score: -30.95281219482422\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 251]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁information\n",
      "--------------\n",
      "score: -31.014822006225586\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 8]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁the\n",
      "--------------\n",
      "score: -31.416351318359375\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 1147, 16, 8, 606, 13]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁step▁in▁the▁development▁of\n",
      "--------------\n",
      "score: -31.77092933654785\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 821, 13, 8]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁performance▁of▁the\n",
      "--------------\n",
      "score: -31.824724197387695\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 894, 8]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁seen▁the\n",
      "--------------\n",
      "score: -31.854305267333984\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 34, 19, 8, 200, 194, 12]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁it▁is▁the▁best▁way▁to\n",
      "--------------\n",
      "score: -31.90481185913086\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 1147, 16, 8, 433, 13]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁step▁in▁the▁process▁of\n",
      "--------------\n",
      "score: -31.947189331054688\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 8, 251]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁the▁information\n",
      "--------------\n",
      "score: -31.97599983215332\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 772]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁results\n",
      "--------------\n",
      "score: -31.980226516723633\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 1295, 12]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁approach▁to\n",
      "--------------\n",
      "score: -32.027374267578125\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 612, 48]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁done▁this\n",
      "--------------\n",
      "score: -32.043724060058594\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 915, 46, 677, 13]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁present▁an▁example▁of\n",
      "--------------\n",
      "score: -32.06929397583008\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 5970, 24, 48, 19, 8, 200, 194, 12]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁demonstrate▁that▁this▁is▁the▁best▁way▁to\n",
      "--------------\n",
      "score: -32.14604949951172\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 915, 8, 2077, 13]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁present▁the▁concept▁of\n",
      "--------------\n",
      "score: -32.150856018066406\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 8, 772]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁the▁results\n",
      "--------------\n",
      "score: -32.21244430541992\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 24, 62, 43, 1597]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁that▁we▁have▁developed\n",
      "--------------\n",
      "score: -32.21251678466797\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 48, 19, 8, 200, 194, 12]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁this▁is▁the▁best▁way▁to\n",
      "--------------\n",
      "score: -32.25581359863281\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 915, 8, 772, 13]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁present▁the▁results▁of\n",
      "--------------\n",
      "score: -32.29237365722656\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 118, 338]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁been▁using\n",
      "--------------\n",
      "score: -32.390594482421875\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 821, 13, 8, 331]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁performance▁of▁the▁data\n",
      "--------------\n",
      "score: -32.39433288574219\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 261, 8]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁used▁the\n",
      "--------------\n",
      "score: -32.4228630065918\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 161]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁work\n",
      "--------------\n",
      "score: -32.42914962768555\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4277, 8, 2077, 13]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁introduce▁the▁concept▁of\n",
      "--------------\n",
      "score: -32.50032043457031\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 48, 19, 8, 495, 28, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁this▁is▁the▁case▁with▁the\n",
      "--------------\n",
      "score: -32.52867889404297\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 24, 54, 36, 261, 21]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁that▁can▁be▁used▁for\n",
      "--------------\n",
      "score: -32.53289794921875\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 5970, 24, 48, 19, 8, 495, 28, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁demonstrate▁that▁this▁is▁the▁case▁with▁the\n",
      "--------------\n",
      "score: -32.549312591552734\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 24, 62, 43, 261]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁that▁we▁have▁used\n",
      "--------------\n",
      "score: -32.55836868286133\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 24, 54, 36, 2930, 12]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁that▁can▁be▁applied▁to\n",
      "--------------\n",
      "score: -32.56642532348633\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 24, 62, 43, 894]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁that▁we▁have▁seen\n",
      "--------------\n",
      "score: -32.5990104675293\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 48, 19, 8, 166, 1147, 16]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁this▁is▁the▁first▁step▁in\n",
      "--------------\n",
      "score: -32.60602569580078\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 24, 62, 43, 118]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁that▁we▁have▁been\n",
      "--------------\n",
      "score: -32.61021041870117\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 915, 8, 772, 13, 8, 1693, 13, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁present▁the▁results▁of▁the▁analysis▁of▁the\n",
      "--------------\n",
      "score: -32.64234161376953\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 39, 331]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁your▁data\n",
      "--------------\n",
      "score: -32.65464782714844\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 894, 48]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁seen▁this\n",
      "--------------\n",
      "score: -32.65743637084961\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 1023]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁image\n",
      "--------------\n",
      "score: -32.673744201660156\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 24, 54, 36, 261, 16]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁that▁can▁be▁used▁in\n",
      "--------------\n",
      "score: -32.77650833129883\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 331]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁data\n",
      "--------------\n",
      "score: -32.82637023925781\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 8, 215, 5, 100, 19, 8, 166, 97]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁the▁year.▁This▁is▁the▁first▁time\n",
      "--------------\n",
      "score: -32.8970832824707\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 5970, 24, 48, 19, 8, 495, 16, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁demonstrate▁that▁this▁is▁the▁case▁in▁the\n",
      "--------------\n",
      "score: -32.94544219970703\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 1693]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁analysis\n",
      "--------------\n",
      "score: -32.950096130371094\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 8, 1693]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁the▁analysis\n",
      "--------------\n",
      "score: -32.952003479003906\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 331]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁data\n",
      "--------------\n",
      "score: -32.96937561035156\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 34, 19, 487, 12, 169, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁it▁is▁possible▁to▁use▁the\n",
      "--------------\n",
      "score: -33.021183013916016\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 821, 13, 8, 358]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁performance▁of▁the▁system\n",
      "--------------\n",
      "score: -33.075889587402344\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 1383]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁images\n",
      "--------------\n",
      "score: -33.09077835083008\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 1499]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁text\n",
      "--------------\n",
      "score: -33.09551239013672\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 3911]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁output\n",
      "--------------\n",
      "score: -33.096900939941406\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 48, 19, 8, 495, 16, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁this▁is▁the▁case▁in▁the\n",
      "--------------\n",
      "score: -33.11986541748047\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 12]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁to\n",
      "--------------\n",
      "score: -33.18366622924805\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 69, 331]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁our▁data\n",
      "--------------\n",
      "score: -33.19038772583008\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 358]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁system\n",
      "--------------\n",
      "score: -33.1923942565918\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 24]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁that\n",
      "--------------\n",
      "score: -33.237552642822266\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 39]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁your\n",
      "--------------\n",
      "score: -33.248085021972656\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 251]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁information\n",
      "--------------\n",
      "score: -33.254486083984375\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 16, 8, 296, 12]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁in▁the▁world▁to\n",
      "--------------\n",
      "score: -33.35194778442383\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 2433, 1295]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁alternative▁approach\n",
      "--------------\n",
      "score: -33.37321090698242\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 738]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁content\n",
      "--------------\n",
      "score: -33.37323760986328\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 24, 62, 43, 612]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁that▁we▁have▁done\n",
      "--------------\n",
      "score: -33.375091552734375\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 821, 13, 8, 825]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁performance▁of▁the▁model\n",
      "--------------\n",
      "score: -33.38705062866211\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 3785]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁input\n",
      "--------------\n",
      "score: -33.390052795410156\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 7452, 13, 8, 1023]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁accuracy▁of▁the▁image\n",
      "--------------\n",
      "score: -33.393714904785156\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 21]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁for\n",
      "--------------\n",
      "score: -33.41572570800781\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 16, 8, 296, 24]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁in▁the▁world▁that\n",
      "--------------\n",
      "score: -33.42208480834961\n",
      "generated ids [32099, 24, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 69]\n",
      "generated text <extra_id_0>▁that▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁our\n",
      "--------------\n",
      "score: -33.422462463378906\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 5, 100, 19, 8, 166, 97]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper.▁This▁is▁the▁first▁time\n",
      "--------------\n",
      "score: -33.46770477294922\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 62, 43]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁we▁have\n",
      "--------------\n",
      "score: -33.471004486083984\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 118, 464]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁been▁working\n",
      "--------------\n",
      "score: -33.481773376464844\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 8, 215, 5, 100, 19, 8, 166, 1147]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁the▁year.▁This▁is▁the▁first▁step\n",
      "--------------\n",
      "score: -33.49677276611328\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 261, 48]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁used▁this\n",
      "--------------\n",
      "score: -33.51164245605469\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 34, 19, 487, 12, 1984, 8]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁it▁is▁possible▁to▁achieve▁the\n",
      "--------------\n",
      "score: -33.51538848876953\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 4230, 46, 12628, 24, 54, 36, 261, 38]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁propose▁an▁algorithm▁that▁can▁be▁used▁as\n",
      "--------------\n",
      "score: -33.52101135253906\n",
      "generated ids [32099, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 34, 19, 8, 167, 1231, 194]\n",
      "generated text <extra_id_0>▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁it▁is▁the▁most▁effective▁way\n",
      "--------------\n",
      "score: -33.52845001220703\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 1597, 8]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁developed▁the\n",
      "--------------\n",
      "score: -33.53371810913086\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 166, 985, 13, 48, 1040, 6, 62, 504, 24, 48, 19]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁first▁half▁of▁this▁paper,▁we▁show▁that▁this▁is\n",
      "--------------\n",
      "score: -33.54457092285156\n",
      "generated ids [32099, 21, 8, 166, 97, 16, 8, 296, 5, 100, 19, 8, 166, 97, 62, 43, 5285, 8]\n",
      "generated text <extra_id_0>▁for▁the▁first▁time▁in▁the▁world.▁This▁is▁the▁first▁time▁we▁have▁tested▁the\n",
      "--------------\n",
      "score: -33.584232330322266\n",
      "generated ids [32099, 54, 36, 261, 12, 1172, 8, 463, 13, 8, 331, 11, 1172, 8, 463, 13, 8, 17953]\n",
      "generated text <extra_id_0>▁can▁be▁used▁to▁improve▁the▁quality▁of▁the▁data▁and▁improve▁the▁quality▁of▁the▁dataset\n",
      "####### generated results #######\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"*cls**sentu_0**<extra_id_0>**label**<extra_id_1>**sep+*\"\n",
    "generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None, truncate='tail')[:beam//2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c445c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Single sentence tasks\n",
    "# We take two kinds of templates: put [MASK] at the beginning or the end\n",
    "template = \"*cls**sentu_0**<extra_id_0>**label**<extra_id_1>**sep+*\"\n",
    "#generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None, truncate='head')[:beam//2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb7e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_number=2\n",
    "label=None\n",
    "length_limit=None\n",
    "truncate='head'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ded79150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic image segmentation\n",
      "volumetric medical image segmentation\n",
      "squad\n",
      "interactions of two sentences\n",
      "shogi\n",
      "large pose 3d face reconstruction\n",
      "leakgan\n",
      "coco object detection dataset\n",
      "abstract scenes\n",
      "svhn\n",
      "dense human pose estimation\n",
      "atari domain\n",
      "svhn\n",
      "text classification\n",
      "snli\n",
      "visual object tracking\n",
      "segmentation\n",
      "sje\n",
      "cifar - 10\n",
      "sr\n",
      "transformer\n",
      "cifar - 10\n",
      "cascade r - cnn\n",
      "optical flow estimation\n",
      "reactor\n",
      "deep gmms\n",
      "example - based single image super resolution\n",
      "weakly supervised object detection\n",
      "ghostvlad\n",
      "conll - 2005\n",
      "fast rigid object detection\n",
      "cifar - 10\n",
      "aspect - based sentiment analysis\n",
      "cifar - 10\n",
      "image super - resolution\n",
      "mr\n",
      "u - net\n",
      "neural machine translation\n",
      "map aerial photo\n",
      "real - world cityscapes dataset\n",
      "neural machine translation\n",
      "gps data\n",
      "arc - ii\n",
      "mscoco\n",
      "named entity recognition\n",
      "ms coco\n",
      "person re - identification\n",
      "ms - marco\n",
      "imagenet\n",
      "dpn - 131\n",
      "nonlinear\n",
      "m - walk\n",
      "sr\n",
      "fpn\n",
      "- shot\n",
      "machine translation\n",
      "wn18\n",
      "ebm - nlp\n",
      "street view house numbers\n",
      "transformer\n",
      "statistical machine translation\n",
      "div2 k dataset\n",
      "multimodal sentiment analysis\n",
      "cifar - 100\n",
      "ptb\n",
      "sr\n",
      "hard\n",
      "person re - i d\n",
      "auxiliary overclustering head\n",
      "character - level language modelling\n",
      "qm9 dataset\n",
      "pascal voc 2012\n",
      "cifar\n",
      "machine translation\n",
      "flownet2 - s\n",
      "camvid dataset\n",
      "snli\n",
      "cifar - 10\n",
      "cifar - 100\n",
      "movielens 10 m\n",
      "atari 2600 games\n",
      "robust04\n",
      "depiction invariant object recognition\n",
      "video - based facial expression recognition\n",
      "human pose estimation\n",
      "sarsa - - eb\n",
      "cifar - 100\n",
      "mnist\n",
      "keypoints\n",
      "conll’09 datasets\n",
      "cifar - 10\n",
      "qasent\n",
      "oxford5k\n",
      "cifar - 10\n",
      "cnn , daily mail\n",
      "visual question answering\n",
      "memn2n\n",
      "text classification\n",
      "- shot\n",
      "ilsvrc13 detection dataset\n",
      "word - level language modeling\n",
      "tucker\n",
      "snli\n",
      "triviaqa version1.0\n",
      "wdw - strict\n",
      "part - based r - cnns\n",
      "fce dataset\n",
      "word\n",
      "person re - identification\n",
      "- shot\n",
      "transformer\n",
      "named entity recognition\n",
      "company\n",
      "neural machine translation\n",
      "pydci\n",
      "fpnn\n",
      "is\n",
      "french - english\n",
      "market - 1501\n",
      "machine translation\n",
      "human pose estimation\n",
      "click - through rate prediction\n",
      "conversational speech recognition\n",
      "arbitraryoriented scene text detection\n",
      "english texts\n",
      "at\n",
      "compcars ”\n",
      "market - 1501\n",
      "question well - formedness classification\n",
      "conll 2005\n",
      "gorila dqn\n",
      "mask r - cnn\n",
      "wavenet\n",
      "english romanian\n",
      "blogcatalog\n",
      "ptb\n",
      "parsenet\n",
      "key - value memory networks\n",
      "morph - 2\n",
      "deep attention selective networks\n",
      "click - through\n",
      "daily mail\n",
      "pascal voc 2007\n",
      "pose estimation\n",
      "vqa\n",
      "sun - rgbd\n",
      "deepid2\n",
      "babi\n",
      "image super - resolution\n",
      "celeba\n",
      "part - of - speech\n",
      "retrieval\n",
      "mrnns\n",
      "pixelrnns\n",
      "deep residual networks\n",
      "perplexity\n",
      "person re - identification\n",
      "ihdp\n",
      "cifar - 10\n",
      "cifar - 10 without dropout\n",
      "gec\n",
      "optical flow estimation\n",
      "mnist\n",
      "image question answering\n",
      "ghm - c\n",
      "is\n",
      "inception score\n",
      "switchboard\n",
      "masque\n",
      "cifar10\n",
      "spinn\n",
      "human head pose estimation\n",
      "mult - vae pr\n",
      "stanford cars\n",
      "image super - resolution\n",
      "pain intensity regression\n",
      "scale - forecast network\n",
      "age - invariant face recognition\n",
      "location - aware networks\n",
      "snli\n",
      "6d object pose estimation\n",
      "market - 1501\n",
      "r2u - net\n",
      "summarization\n",
      "machine translation\n",
      "exam\n",
      "pix2pix\n",
      "portuguese\n",
      "cifar - 100\n",
      "fast r - cnn\n",
      "nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/211679273.tmpdir/ipykernel_97269/2852227745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'head'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/211679273.tmpdir/ipykernel_97269/3792710870.py\u001b[0m in \u001b[0;36mget_text\u001b[0;34m(template, input_text_tuple, label, tokenizer, mapping)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnew_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_token_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnew_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sent_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Generate templates based on given inputs\n",
    "\n",
    "label: Only use instances with this label (deprecated)\n",
    "length_limit: At least generate content as long as length_limit (deprecated)\n",
    "\"\"\"\n",
    "input_texts = []\n",
    "input_tensors = []\n",
    "max_length = 0\n",
    "\n",
    "# Process the inputs\n",
    "for item in dataset:\n",
    "    if label is None or item['label'] == label:\n",
    "        if item['label']==\n",
    "        print(item['label'])\n",
    "        input_text = get_text(template, item['text'], item['label'], tokenizer, mapping)\n",
    "        if truncate is not None:\n",
    "            if truncate == 'head':\n",
    "                input_text = input_text[-256:]\n",
    "            elif truncate == 'tail':\n",
    "                input_text = input_text[:256]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        input_ids = torch.tensor(input_text).long()\n",
    "        max_length = max(max_length, input_ids.size(-1))\n",
    "        input_tensors.append(input_ids)\n",
    "\n",
    "# Concatenate inputs as a batch\n",
    "input_ids = torch.zeros((len(input_tensors), max_length)).long()\n",
    "attention_mask = torch.zeros((len(input_tensors), max_length)).long()\n",
    "for i in range(len(input_tensors)):\n",
    "    input_ids[i, :input_tensors[i].size(-1)] = input_tensors[i]\n",
    "    attention_mask[i, :input_tensors[i].size(-1)] = 1\n",
    "\n",
    "# Print some examples\n",
    "print('####### example #######')\n",
    "print(tokenizer.decode(input_ids[0]))\n",
    "print(tokenizer.decode(input_ids[1]))\n",
    "print(tokenizer.decode(input_ids[2]))\n",
    "print('####### example #######\\n')\n",
    "\n",
    "input_ids = input_ids.cuda()\n",
    "attention_mask = attention_mask.cuda()\n",
    "assert len(input_tensors) > 0\n",
    "\n",
    "# Maximum generate content length\n",
    "max_length = 20\n",
    "\n",
    "start_mask = tokenizer._convert_token_to_id('<extra_id_0>')\n",
    "ori_decoder_input_ids = torch.zeros((input_ids.size(0), max_length)).long()\n",
    "ori_decoder_input_ids[..., 0] = model.config.decoder_start_token_id\n",
    "\n",
    "# decoder_input_ids: decoder inputs for next regressive generation\n",
    "# ll: log likelihood\n",
    "# output_id: which part of generated contents we are at\n",
    "# output: generated content so far\n",
    "# last_length (deprecated): how long we have generated for this part\n",
    "current_output = [{'decoder_input_ids': ori_decoder_input_ids, 'll': 0, 'output_id': 1, 'output': [], 'last_length': -1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d7a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cec62a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.isnan(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbe4711",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/211679273.tmpdir/ipykernel_97269/2103620212.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "item['label']==math.isnan(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9219dbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f4907b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71,\n",
       " 3326,\n",
       " 120,\n",
       " 529,\n",
       " 7,\n",
       " 35,\n",
       " 7,\n",
       " 1950,\n",
       " 12373,\n",
       " 81,\n",
       " 3,\n",
       " 9,\n",
       " 690,\n",
       " 1107,\n",
       " 3943,\n",
       " 44,\n",
       " 165,\n",
       " 13846,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 32099,\n",
       " 248,\n",
       " 32098,\n",
       " 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7f9f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25c82304",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "new_current_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11c81179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_input_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'll': 0, 'output_id': 1, 'output': [], 'last_length': -1}\n"
     ]
    }
   ],
   "source": [
    "for item in current_output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21d6bbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_input_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'll': 0,\n",
       " 'output_id': 1,\n",
       " 'output': [],\n",
       " 'last_length': -1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edd2af6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2bc1361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (3584501203.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/scratch/211657848.tmpdir/ipykernel_93795/3584501203.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "if item['output_id'] > target_number:\n",
    "    # Enough contents\n",
    "    new_current_output.append(item)\n",
    "    continue\n",
    "decoder_input_ids = item['decoder_input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ab5bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = item['decoder_input_ids']\n",
    "\n",
    "# Forward\n",
    "batch_size = 32\n",
    "turn = input_ids.size(0) // batch_size\n",
    "if input_ids.size(0) % batch_size != 0:\n",
    "    turn += 1\n",
    "aggr_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca4ce410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "639f2518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae6821f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d68fb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6920"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0bad513",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids1=input_ids[start:end]\n",
    "attention_mask1=attention_mask[start:end]\n",
    "decoder_input_ids1=decoder_input_ids.cuda()[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "293aeeee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/211657848.tmpdir/ipykernel_93795/2652744722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mop1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1563\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             )\n\u001b[1;32m   1567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 )\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         )\n\u001b[1;32m    552\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# get query states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# get key/value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "op1=model(input_ids1, attention_mask1, decoder_input_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583a305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaee977",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_output.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce9dbd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/211657848.tmpdir/ipykernel_93795/875637434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0maggr_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0maggr_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1563\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             )\n\u001b[1;32m   1567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 )\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         )\n\u001b[1;32m    552\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# get query states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# get key/value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "for item in current_output:\n",
    "    print(\"a\")\n",
    "    if item['output_id'] > target_number:\n",
    "        # Enough contents\n",
    "        new_current_output.append(item)\n",
    "        continue\n",
    "    decoder_input_ids = item['decoder_input_ids']\n",
    "\n",
    "    # Forward\n",
    "    batch_size = 32\n",
    "    turn = input_ids.size(0) // batch_size\n",
    "    if input_ids.size(0) % batch_size != 0:\n",
    "        turn += 1\n",
    "    aggr_output = []\n",
    "    for t in range(turn):\n",
    "        start = t * batch_size\n",
    "        end = min((t + 1) * batch_size, input_ids.size(0))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            aggr_output.append(model(input_ids[start:end], attention_mask=attention_mask[start:end], decoder_input_ids=decoder_input_ids.cuda()[start:end])[0])\n",
    "    aggr_output = torch.cat(aggr_output, 0)\n",
    "\n",
    "    # Gather results across all input sentences, and sort generated tokens by log likelihood\n",
    "    aggr_output = aggr_output.mean(0)\n",
    "    log_denominator = torch.logsumexp(aggr_output[i], -1).item()\n",
    "    ids = list(range(model.config.vocab_size))\n",
    "    ids.sort(key=lambda x: aggr_output[i][x].item(), reverse=True)\n",
    "    ids = ids[:beam+3]\n",
    "\n",
    "    for word_id in ids:\n",
    "        output_id = item['output_id']\n",
    "\n",
    "        if word_id == start_mask - output_id or word_id == tokenizer._convert_token_to_id('</s>'):\n",
    "            # Finish one part\n",
    "            if length_limit is not None and item['last_length'] < length_limit[output_id - 1]:\n",
    "                check = False\n",
    "            else:\n",
    "                check = True\n",
    "            output_id += 1\n",
    "            last_length = 0\n",
    "        else:\n",
    "            last_length = item['last_length'] + 1\n",
    "            check = True\n",
    "\n",
    "        output_text = item['output'] + [word_id]\n",
    "        ll = item['ll'] + aggr_output[i][word_id] - log_denominator\n",
    "        new_decoder_input_ids = decoder_input_ids.new_zeros(decoder_input_ids.size())\n",
    "        new_decoder_input_ids[:] = decoder_input_ids\n",
    "        new_decoder_input_ids[..., i + 1] = word_id\n",
    "\n",
    "        # Forbid single space token, \"....\", and \"..........\"\n",
    "        if word_id in [3, 19794, 22354]:\n",
    "            check = False\n",
    "\n",
    "        # Forbid continuous \".\"\n",
    "        if len(output_text) > 1 and output_text[-2] == 5 and output_text[-1] == 5:\n",
    "            check = False\n",
    "\n",
    "        if check:\n",
    "            # Add new results to beam search pool\n",
    "            new_item = {'decoder_input_ids': new_decoder_input_ids, 'll': ll, 'output_id': output_id, 'output': output_text, 'last_length': last_length}\n",
    "            new_current_output.append(new_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec166e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Forward\n",
    "batch_size = 32\n",
    "turn = input_ids.size(0) // batch_size\n",
    "if input_ids.size(0) % batch_size != 0:\n",
    "    turn += 1\n",
    "aggr_output = []\n",
    "for t in range(turn):\n",
    "    start = t * batch_size\n",
    "    end = min((t + 1) * batch_size, input_ids.size(0))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        aggr_output.append(model(input_ids[start:end], attention_mask=attention_mask[start:end], decoder_input_ids=decoder_input_ids.cuda()[start:end])[0])\n",
    "aggr_output = torch.cat(aggr_output, 0)\n",
    "\n",
    "# Gather results across all input sentences, and sort generated tokens by log likelihood\n",
    "aggr_output = aggr_output.mean(0)\n",
    "log_denominator = torch.logsumexp(aggr_output[i], -1).item()\n",
    "ids = list(range(model.config.vocab_size))\n",
    "ids.sort(key=lambda x: aggr_output[i][x].item(), reverse=True)\n",
    "ids = ids[:beam+3]\n",
    "\n",
    "for word_id in ids:\n",
    "    output_id = item['output_id']\n",
    "\n",
    "    if word_id == start_mask - output_id or word_id == tokenizer._convert_token_to_id('</s>'):\n",
    "        # Finish one part\n",
    "        if length_limit is not None and item['last_length'] < length_limit[output_id - 1]:\n",
    "            check = False\n",
    "        else:\n",
    "            check = True\n",
    "        output_id += 1\n",
    "        last_length = 0\n",
    "    else:\n",
    "        last_length = item['last_length'] + 1\n",
    "        check = True\n",
    "\n",
    "    output_text = item['output'] + [word_id]\n",
    "    ll = item['ll'] + aggr_output[i][word_id] - log_denominator\n",
    "    new_decoder_input_ids = decoder_input_ids.new_zeros(decoder_input_ids.size())\n",
    "    new_decoder_input_ids[:] = decoder_input_ids\n",
    "    new_decoder_input_ids[..., i + 1] = word_id\n",
    "\n",
    "    # Forbid single space token, \"....\", and \"..........\"\n",
    "    if word_id in [3, 19794, 22354]:\n",
    "        check = False\n",
    "\n",
    "    # Forbid continuous \".\"\n",
    "    if len(output_text) > 1 and output_text[-2] == 5 and output_text[-1] == 5:\n",
    "        check = False\n",
    "\n",
    "    if check:\n",
    "        # Add new results to beam search pool\n",
    "        new_item = {'decoder_input_ids': new_decoder_input_ids, 'll': ll, 'output_id': output_id, 'output': output_text, 'last_length': last_length}\n",
    "        new_current_output.append(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if len(new_current_output) == 0:\n",
    "    break\n",
    "\n",
    "new_current_output.sort(key=lambda x: x['ll'], reverse=True)\n",
    "new_current_output = new_current_output[:beam]\n",
    "current_output = new_current_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "result = []\n",
    "print(\"####### generated results #######\")\n",
    "for item in current_output:\n",
    "    generate_text = ''\n",
    "    for token in item['output']:\n",
    "        generate_text += tokenizer._convert_id_to_token(token)\n",
    "    print('--------------')\n",
    "    print('score:', item['ll'].item())\n",
    "    print('generated ids', item['output'])\n",
    "    print('generated text', generate_text)\n",
    "    result.append(generate_text)\n",
    "print(\"####### generated results #######\\n\")\n",
    "\n",
    "return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cca7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### example #######\n",
      "A stirring, funny and finally transporting re-imagining of beauty and the beast and 1930s horror films<extra_id_0> great<extra_id_1> </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Apparently reassembled from the cutting-room floor of any given daytime soap.<extra_id_0> terrible<extra_id_1> </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "They presume their audience won't sit still for a sociology lesson, however entertainingly presented, so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes.<extra_id_0> terrible<extra_id_1> </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "####### example #######\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/211657848.tmpdir/ipykernel_13305/2631033852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"####### generated templates #######\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Transform T5 outputs to our template format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/211657848.tmpdir/ipykernel_13305/2153475465.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(dataset, template, model, tokenizer, target_number, mapping, beam, label, length_limit, truncate)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                     \u001b[0maggr_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0maggr_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1563\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             )\n\u001b[1;32m   1567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return F.embedding(\n\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None, truncate='head')[:beam//2]\n",
    "\n",
    "print(\"####### generated templates #######\")\n",
    "for text in generate_text:\n",
    "    # Transform T5 outputs to our template format\n",
    "    text = text.replace('<extra_id_0>', '*cls**sent_0*')\n",
    "    text = text.replace('<extra_id_1>', '*mask*')\n",
    "    text = text.replace('<extra_id_2>', '*sep+*')\n",
    "    text = text.replace('</s>', '*sep+*')\n",
    "    text = text.replace('▁', '_')\n",
    "    print(text)\n",
    "    f.write(text + '\\n')\n",
    "print(\"####### generated templates #######\\n\")\n",
    "\n",
    "template = \"*cls*.*<extra_id_0>**label**<extra_id_1>**+sentu_0**sep+*\"\n",
    "generate_text = generate(dataset, template, model, tokenizer, target_number=2, mapping=mapping, beam=beam, label=None, truncate='tail')[:beam//2]\n",
    "print(\"####### generated templates #######\")\n",
    "for text in generate_text:\n",
    "    # Transform T5 outputs to our template format\n",
    "    text = text.replace('<extra_id_0>', '*cls*')\n",
    "    text = text.replace('<extra_id_1>', '*mask*')\n",
    "    text = text.replace('<extra_id_2>', '*+sent_0**sep+*')\n",
    "    text = text.replace('</s>', '*+sent_0**sep+*')\n",
    "    text = text.replace('▁', '_')\n",
    "    print(text)\n",
    "    f.write(text + '\\n')\n",
    "print(\"####### generated templates #######\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e45fc978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9aa002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
